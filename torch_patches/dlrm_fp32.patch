From 937b5c9632aa68ef8498cd69a4c89240847a0276 Mon Sep 17 00:00:00 2001
From: "Chen, Jian Ping" <jian.ping.chen@intel.com>
Date: Thu, 23 Apr 2020 22:13:54 +0800
Subject: [PATCH] DLRM Patches

---
 aten/src/ATen/native/EmbeddingBag.cpp         | 159 ++++++++++++++--
 aten/src/ATen/native/EmbeddingBag.h           |  14 ++
 aten/src/ATen/native/TensorIterator.cpp       |   8 +-
 aten/src/ATen/native/TensorIterator.h         |   5 +-
 aten/src/ATen/native/cpu/AtomicAddFloat.h     |  33 ++++
 aten/src/ATen/native/cpu/IndexKernel.cpp      |  24 ++-
 aten/src/ATen/native/cpu/Loops.h              |   4 +-
 aten/src/ATen/native/cuda/EmbeddingBag.cu     |  45 +++++
 aten/src/ATen/native/native_functions.yaml    |   3 +
 .../ATen/native/sparse/SparseTensorMath.cpp   | 180 +++++++++++++++---
 aten/src/TH/generic/THTensorApply.hpp         |   2 +-
 11 files changed, 416 insertions(+), 61 deletions(-)
 create mode 100644 aten/src/ATen/native/EmbeddingBag.h
 create mode 100644 aten/src/ATen/native/cpu/AtomicAddFloat.h

diff --git a/aten/src/ATen/native/EmbeddingBag.cpp b/aten/src/ATen/native/EmbeddingBag.cpp
index 29cf2da698..07b469e8ec 100644
--- a/aten/src/ATen/native/EmbeddingBag.cpp
+++ b/aten/src/ATen/native/EmbeddingBag.cpp
@@ -2,6 +2,7 @@
 #include <ATen/NativeFunctions.h>
 #include <ATen/Parallel.h>
 #include <ATen/TensorUtils.h>
+#include <ATen/native/EmbeddingBag.h>
 
 #include <TH/THBlasUtils.h>
 
@@ -24,20 +25,13 @@ namespace {
 namespace at {
 namespace native {
 
-static void make_offset2bag(const Tensor &offsets, const Tensor &indices, Tensor& offset2bag) {
-  offset2bag.index_add_(
-      0, offsets, at::ones_like(offsets, LEGACY_CONTIGUOUS_MEMORY_FORMAT)); // offset2bag = [1 0 1 0 1]
-  offset2bag[0] -= 1;                     // offset2bag = [0 0 1 0 1]
-  offset2bag = offset2bag.cumsum(0);     // offset2bag = [0 0 1 1 2]
-}
-
 namespace {
 
-bool isFastPathIndexSelect(const Tensor& src, Tensor& output) {
+static inline bool isFastPathIndexSelect(const Tensor& src, Tensor& output) {
   return src.scalar_type() == kFloat && src.stride(1) == 1 && output.stride(1) == 1;
 }
 
-bool isFastPathIndexSelectScale(const Tensor& src, const Tensor& scale, Tensor& output) {
+static inline bool isFastPathIndexSelectScale(const Tensor& src, const Tensor& scale, Tensor& output) {
   return src.scalar_type() == kFloat && src.stride(1) == 1 && output.stride(1) == 1 && scale.stride(0) == 1;
 }
 
@@ -87,21 +81,19 @@ void index_select_add<float>(const Tensor &select_indices,
     auto* offsets_data = offsets.data_ptr<int64_t>();
     std::vector<int64_t> offsets_include_last;
 
-    if (include_last_offset) {
-      output_size = offsets.numel() - 1;
-    } else {
+    if (!include_last_offset) {
       output_size = offsets.numel();
-      offsets_include_last.resize(offsets.numel() + 1);
+      offsets_include_last.resize(output_size + 1);
       std::memcpy(
           offsets_include_last.data(),
-          offsets.data_ptr<int64_t>(),
-          sizeof(int64_t) * offsets.numel());
-      offsets_include_last[offsets.numel()] = select_indices.numel();
+          offsets_data,
+          sizeof(int64_t) * output_size);
+      offsets_include_last[output_size] = select_indices.numel();
       offsets_data = offsets_include_last.data();
     }
 
     at::parallel_for(
-        0, output_size, 1, [&](int64_t start_idx, int64_t end_idx) {
+        0, output_size, 64, [&](int64_t start_idx, int64_t end_idx) {
           caffe2::EmbeddingLookupIdx(
               /*block_size=*/ddim,
               /*output_size=*/end_idx - start_idx,
@@ -459,9 +451,132 @@ _embedding_bag_cpu(const Tensor &weight, const Tensor &indices,
   }
 }
 
+Tensor _embedding_bag_sparse_backward_cpu_sum_fast(
+    const Tensor &grad, const Tensor &indices, const Tensor &offsets, int64_t num_weights, int64_t mode, const Tensor& per_sample_weights) {
+
+  AT_ASSERT(mode == MODE_SUM);
+  AT_ASSERT((grad.scalar_type() == kFloat)&& (grad.stride(1) == 1) && !per_sample_weights.defined());
+
+  int64_t indices_size0 = indices.size(0);
+  int64_t ddim = grad.size(1);
+  Tensor index_grad = at::empty({indices_size0, ddim}, grad.options());
+  float* gradout_data = index_grad.data_ptr<float>();
+
+  auto offsets_accessor = offsets.accessor<int64_t, 1>();
+  auto offset_numel = offsets.numel();
+
+  float* grad_data = grad.data_ptr<float>();
+  int grad_stride0 = grad.stride(0);
+  at::parallel_for(0, offset_numel, 0, [&](int64_t start, int64_t end) {
+    for(auto mb = start; mb < end; mb++) {
+      int64_t select_off_start = offsets_accessor[mb];
+      int64_t select_off_end = (mb < (offset_numel - 1) ? offsets_accessor[mb + 1] : indices_size0);
+      auto grad_block = grad_data + grad_stride0 * mb;;
+      for (int64_t s = select_off_start; s < select_off_end; s++) {
+        THBlas_copy<float>(ddim, grad_block, 1, gradout_data + ddim * s, 1);
+      }
+    }
+  });
+
+  int64_t num_features = index_grad.size(-1);
+  auto weight_size = std::array<int64_t, 2>{{ num_weights, num_features }};
+  auto dense_options = index_grad.options();
+
+  if (index_grad.numel() == 0) {
+    return at::_sparse_coo_tensor_unsafe(at::empty({1, 0}, indices.options()),
+                                         at::empty({0, num_features}, dense_options),
+                                         weight_size);
+  }
+
+  auto index = indices.reshape({1, -1});
+  auto values = index_grad.reshape({-1, num_features});
+
+  return at::_sparse_coo_tensor_unsafe(index, values, weight_size);
+
+}
+
+Tensor _embedding_bag_dense_backward_cpu_sum_fast(
+    const Tensor &grad, const Tensor &indices, const Tensor &offsets, int64_t num_weights, int64_t mode, const Tensor& per_sample_weights) {
+
+  AT_ASSERT(mode == MODE_SUM);
+  AT_ASSERT((grad.scalar_type() == kFloat)&& (grad.stride(1) == 1) && !per_sample_weights.defined());
+
+  int64_t indices_numel = indices.numel();
+  auto offset_numel = offsets.numel();
+
+  Tensor offset2bag;
+  if (indices_numel != offset_numel) {
+    offset2bag = at::zeros(
+      {indices.sizes()[0] + 1}, indices.options()); // offset2bag = [0 0 0 0 0]
+    make_offset2bag(offsets, indices, offset2bag);
+    offset2bag.resize_({indices.sizes()[0]});
+  } else {
+    offset2bag = offsets;
+  }
+
+  int64_t ddim = grad.size(1);
+  Tensor index_grad_weight = at::zeros({num_weights, ddim}, grad.options());
+
+  int64_t grad_length = index_grad_weight.size(0);
+  int max_threads = at::get_num_threads();
+  max_threads = (grad_length < max_threads) ? grad_length : max_threads;
+  int64_t avg_chunk_down = grad_length / max_threads;
+  int64_t chuck_size[max_threads];
+  for (auto i = 0; i < max_threads; i++) {
+    chuck_size[i] = avg_chunk_down;
+  }
+  //make chunk balance among threads as 211
+  for (auto i = 0 ; i < grad_length % max_threads ; i++) {
+    chuck_size[i] += 1;
+  }
+  int64_t chuck_sum_size[max_threads + 1];
+  chuck_sum_size[0] = 0;
+  for (auto i = 1; i < max_threads; i++) {
+    chuck_sum_size[i] = chuck_sum_size[i - 1] + chuck_size[i - 1];
+  }
+  chuck_sum_size[max_threads] = grad_length;
+
+  auto* indices_data = indices.data_ptr<int64_t>();
+  auto* offset2bag_data = offset2bag.data_ptr<int64_t>();
+  auto* grad_data = grad.data_ptr<float>();
+  auto* gradout_data = index_grad_weight.data_ptr<float>();
+  int64_t grad_stride0 = grad.stride(0);
+  at::parallel_for(0, max_threads, 0, [&](int64_t start, int64_t end) {
+    for(auto k = start; k < end; k++) {
+      int64_t chunk_start = chuck_sum_size[k];
+      int64_t chunk_end = chuck_sum_size[k + 1];
+      for (int64_t mb = 0; mb < indices_numel; mb++) {
+        int64_t index = indices_data[mb];
+        if (index >= chunk_start && index < chunk_end) {
+          auto s = offset2bag_data[mb];
+          THBlas_axpy<float>(ddim, 1.0, grad_data + grad_stride0 * s, 1, gradout_data + ddim * index, 1);
+        }
+      }
+    }
+  });
+
+  return index_grad_weight;
+}
+
+// To save compute, if we are going to go down the fast path case for the 'sum'
+// mode, we skip calculating offset2bag, since it is not going to be used.
+static inline bool _embedding_bag_fast_path_sum(const Tensor& grad,
+      const Tensor &indices,
+      const Tensor &offset2bag,
+      const Tensor& per_sample_weights,
+       bool scale_grad_by_freq,
+      int64_t mode) {
+
+  if (at::get_num_threads() == 1) return false;
+  if (offset2bag.numel() != 0 || indices.numel() == 0) return false;
+  if (mode != MODE_SUM || grad.scalar_type() != kFloat) return false;
+  if (per_sample_weights.defined() || scale_grad_by_freq) return false;
+  return true;
+}
+
 // Assumes all input tensors are contiguous.
 // See NOTE [ embedding_bag Native Functions ] in native_functions.yaml for details
-Tensor _embedding_bag_backward(const Tensor &grad, const Tensor &indices,
+Tensor _embedding_bag_backward_cpu(const Tensor &grad, const Tensor &indices,
                               const Tensor &offsets,
                               const Tensor &offset2bag,
                               const Tensor &bag_size_,
@@ -477,6 +592,14 @@ Tensor _embedding_bag_backward(const Tensor &grad, const Tensor &indices,
   checkScalarType("embedding_bag", offsets_arg, kLong);
   checkContiguous("embedding_bag", offsets_arg);
 
+  if (_embedding_bag_fast_path_sum(grad, indices, offset2bag, per_sample_weights, scale_grad_by_freq, mode)) {
+    if (sparse) {
+      return _embedding_bag_sparse_backward_cpu_sum_fast(grad.contiguous(), indices, offsets, num_weights, mode, per_sample_weights);
+    } else {
+      return _embedding_bag_dense_backward_cpu_sum_fast(grad.contiguous(), indices, offsets, num_weights, mode, per_sample_weights);
+    }
+  }
+
   Tensor offset2bag_;
   if (indices.numel() != 0 && offset2bag.numel() == 0) {
     offset2bag_ = at::zeros(
diff --git a/aten/src/ATen/native/EmbeddingBag.h b/aten/src/ATen/native/EmbeddingBag.h
new file mode 100644
index 0000000000..eac925c99c
--- /dev/null
+++ b/aten/src/ATen/native/EmbeddingBag.h
@@ -0,0 +1,14 @@
+#include <ATen/ATen.h>
+
+namespace at {
+namespace native {
+
+static inline void make_offset2bag(const Tensor &offsets, const Tensor &indices, Tensor& offset2bag) {
+  offset2bag.index_add_(
+      0, offsets, at::ones_like(offsets, LEGACY_CONTIGUOUS_MEMORY_FORMAT)); // offset2bag = [1 0 1 0 1]
+  offset2bag[0] -= 1;                     // offset2bag = [0 0 1 0 1]
+  offset2bag = offset2bag.cumsum(0);     // offset2bag = [0 0 1 1 2]
+}
+
+}
+}
diff --git a/aten/src/ATen/native/TensorIterator.cpp b/aten/src/ATen/native/TensorIterator.cpp
index 1200713e73..32c9757cab 100644
--- a/aten/src/ATen/native/TensorIterator.cpp
+++ b/aten/src/ATen/native/TensorIterator.cpp
@@ -526,18 +526,18 @@ int TensorIterator::num_reduce_dims() const {
     }                                                                     \
   }
 
-void TensorIterator::for_each(loop_t loop) {
-  for_each(LOOP_WRAPPER(ntensors(), loop));
+void TensorIterator::for_each(loop_t loop, int64_t grain_size) {
+  for_each(LOOP_WRAPPER(ntensors(), loop), grain_size);
 }
 
-void TensorIterator::for_each(loop2d_t loop) {
+void TensorIterator::for_each(loop2d_t loop, int64_t grain_size) {
   int64_t numel = this->numel();
   if (numel == 0) {
     return;
   } else if (numel < internal::GRAIN_SIZE || at::get_num_threads() == 1) {
     return serial_for_each(loop, {0, numel});
   } else {
-    at::parallel_for(0, numel, internal::GRAIN_SIZE, [&](int64_t begin, int64_t end) {
+    at::parallel_for(0, numel, grain_size, [&](int64_t begin, int64_t end) {
       serial_for_each(loop, {begin, end});
     });
   }
diff --git a/aten/src/ATen/native/TensorIterator.h b/aten/src/ATen/native/TensorIterator.h
index 5d66ebb00b..2fc725a861 100644
--- a/aten/src/ATen/native/TensorIterator.h
+++ b/aten/src/ATen/native/TensorIterator.h
@@ -9,6 +9,7 @@
 #include <c10/util/Optional.h>
 #include <ATen/MemoryOverlap.h>
 #include <ATen/NamedTensorUtils.h>
+#include <ATen/Parallel.h>
 
 // TensorIterator is a helper class for element-wise operations, such as
 // arithmetic, comparisons, and trigonometric functions. It handles
@@ -263,8 +264,8 @@ struct CAFFE2_API TensorIterator {
     return c10::fetch_and_cast<T>(op.tensor.scalar_type(), op.data);
   }
 
-  void for_each(loop_t loop);
-  void for_each(loop2d_t loop);
+  void for_each(loop_t loop, int64_t grain_size = at::internal::GRAIN_SIZE);
+  void for_each(loop2d_t loop, int64_t grain_size = at::internal::GRAIN_SIZE);
 
   void parallel_reduce(loop2d_t loop);
 
diff --git a/aten/src/ATen/native/cpu/AtomicAddFloat.h b/aten/src/ATen/native/cpu/AtomicAddFloat.h
new file mode 100644
index 0000000000..aad289bcaf
--- /dev/null
+++ b/aten/src/ATen/native/cpu/AtomicAddFloat.h
@@ -0,0 +1,33 @@
+#ifndef ATOMIC_ADD_FLOAT
+#define ATOMIC_ADD_FLOAT
+
+#if (defined(__x86_64__) || defined(__i386__))
+#include "ATen/native/cpu/Intrinsics.h"
+#else
+#define _mm_pause()
+#endif
+
+#include <atomic>
+
+static inline void cpu_atomic_add_float(float* dst, float fvalue)
+{
+  typedef union {
+    unsigned intV;
+    float floatV;
+  } uf32_t;
+
+  uf32_t new_value, old_value;
+  std::atomic<unsigned>* dst_intV = (std::atomic<unsigned>*)(dst);
+
+  old_value.floatV = *dst;
+  new_value.floatV = old_value.floatV + fvalue;
+
+  unsigned* old_intV = (unsigned*)(&old_value.intV);
+  while (!std::atomic_compare_exchange_strong(dst_intV, old_intV, new_value.intV)) {
+    _mm_pause();
+    old_value.floatV = *dst;
+    new_value.floatV = old_value.floatV + fvalue;
+  }
+}
+
+#endif
diff --git a/aten/src/ATen/native/cpu/IndexKernel.cpp b/aten/src/ATen/native/cpu/IndexKernel.cpp
index ee124d26ec..109925630b 100644
--- a/aten/src/ATen/native/cpu/IndexKernel.cpp
+++ b/aten/src/ATen/native/cpu/IndexKernel.cpp
@@ -6,6 +6,7 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/Parallel.h>
 #include <ATen/cpu/vec256/vec256.h>
+#include <ATen/native/cpu/AtomicAddFloat.h>
 
 namespace at { namespace native {
 namespace {
@@ -62,6 +63,10 @@ void cpu_index_kernel(TensorIterator& iter, IntArrayRef index_size, IntArrayRef
                       const func_t& f, bool serial_execution=false)
 {
   int ntensor = iter.ntensors();
+  // When launch the index parallel version, set a relative samll grain size less than the INTERNAL::GRAIN_SIZE
+  // to make the whole available thread numbers get more balanced work load and a better cache location.
+  // The grain size here is chosen by the op benchmark to overcome the thread launch overhead
+  const int index_parallel_grain_size = 3000;
   auto loop = [&](char** data, const int64_t* strides, int64_t n) {
     auto indexer = Indexer(ntensor - 2, &data[2], &strides[2], index_size, index_stride);
     char* dst = data[0];
@@ -88,7 +93,7 @@ void cpu_index_kernel(TensorIterator& iter, IntArrayRef index_size, IntArrayRef
   if (serial_execution) {
     iter.serial_for_each(loop, {0, iter.numel()});
   } else {
-    iter.for_each(loop);
+    iter.for_each(loop, index_parallel_grain_size);
   }
 }
 
@@ -106,11 +111,18 @@ void index_put_kernel(TensorIterator& iter, IntArrayRef index_size, IntArrayRef
   AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(at::ScalarType::Half, at::ScalarType::Bool, at::ScalarType::BFloat16,
     iter.dtype(), "index_put", [&] {
     if (accumulate) {
-      // TODO: investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,
-      // this needs to be thread-safe.
-      cpu_index_kernel<scalar_t>(iter, index_size, index_stride, [](char* dst, char* src, int64_t offset) {
-        *(scalar_t*)(dst + offset) += *(scalar_t*)src;
-      }, /*serial_execution=*/true);
+      bool use_parallel_for = ((iter.numel() >= internal::GRAIN_SIZE) && (at::get_num_threads() > 1));
+      if (iter.dtype() == at::ScalarType::Float && use_parallel_for) {
+        cpu_index_kernel<float>(iter, index_size, index_stride, [](char* dst, char* src, int64_t offset) {
+          cpu_atomic_add_float((float*)(dst + offset), *(float*)src);
+        });
+      } else {
+        // TODO: investigate parallelization of the accumulate kernel. Unlike the non-accumulate case,
+        // this needs to be thread-safe.
+        cpu_index_kernel<scalar_t>(iter, index_size, index_stride, [](char* dst, char* src, int64_t offset) {
+          *(scalar_t*)(dst + offset) += *(scalar_t*)src;
+        }, /*serial_execution=*/true);
+      }
     } else {
       cpu_index_kernel<scalar_t>(iter, index_size, index_stride, [](char* dst, char* src, int64_t offset) {
         *(scalar_t*)(dst + offset) = *(scalar_t*)src;
diff --git a/aten/src/ATen/native/cpu/Loops.h b/aten/src/ATen/native/cpu/Loops.h
index 14fdefec20..97313dc8f3 100644
--- a/aten/src/ATen/native/cpu/Loops.h
+++ b/aten/src/ATen/native/cpu/Loops.h
@@ -205,7 +205,7 @@ template <typename func_t, typename vec_func_t>
 void cpu_kernel_vec(TensorIterator& iter, func_t&& op, vec_func_t&& vop) {
   using traits = function_traits<func_t>;
   TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);
-
+  const int loop_vec_parallel_grain_size = 1024;
   iter.for_each([&](char** data, const int64_t* strides, int64_t n) {
     if (is_contiguous<traits>(strides)) {
       return vectorized_loop(data, n, 0, std::forward<func_t>(op), std::forward<vec_func_t>(vop));
@@ -219,7 +219,7 @@ void cpu_kernel_vec(TensorIterator& iter, func_t&& op, vec_func_t&& vop) {
         }
       });
     }
-  });
+  }, loop_vec_parallel_grain_size);
   iter.cast_outputs();
 }
 
diff --git a/aten/src/ATen/native/cuda/EmbeddingBag.cu b/aten/src/ATen/native/cuda/EmbeddingBag.cu
index c0724e87d6..2294c85d40 100644
--- a/aten/src/ATen/native/cuda/EmbeddingBag.cu
+++ b/aten/src/ATen/native/cuda/EmbeddingBag.cu
@@ -16,6 +16,7 @@
 #include <thrust/iterator/constant_iterator.h>
 #include <thrust/device_vector.h>
 
+#include <ATen/native/EmbeddingBag.h>
 #include <ATen/native/cuda/EmbeddingBackwardKernel.cuh>
 
 #include <c10/macros/Macros.h>
@@ -316,6 +317,50 @@ _embedding_bag_cuda(const Tensor &weight, const Tensor &indices,
   return std::tuple<Tensor, Tensor, Tensor, Tensor>(output, offset2bag, bag_size, max_indices);
 }
 
+// Assumes all input tensors are contiguous.
+// See NOTE [ embedding_bag Native Functions ] in native_functions.yaml for details
+Tensor _embedding_bag_backward_cuda(const Tensor &grad, const Tensor &indices,
+                              const Tensor &offsets,
+                              const Tensor &offset2bag,
+                              const Tensor &bag_size_,
+                              const Tensor &max_indices_,
+                              int64_t num_weights,
+                              bool scale_grad_by_freq, int64_t mode,
+                              bool sparse,
+                              const Tensor& per_sample_weights) {
+  auto indices_arg = TensorArg(indices, "indices", 1);
+  checkScalarType("embedding_bag", indices_arg, kLong);
+  checkContiguous("embedding_bag", indices_arg);
+  auto offsets_arg = TensorArg(offsets, "offsets", 1);
+  checkScalarType("embedding_bag", offsets_arg, kLong);
+  checkContiguous("embedding_bag", offsets_arg);
+
+  Tensor offset2bag_;
+  if (indices.numel() != 0 && offset2bag.numel() == 0) {
+    offset2bag_ = at::zeros(
+       {indices.sizes()[0] + 1}, indices.options()); // offset2bag = [0 0 0 0 0]
+
+    make_offset2bag(offsets, indices, offset2bag_);
+
+    offset2bag_.resize_({indices.sizes()[0]});
+  } else {
+    auto offset2bag_arg = TensorArg(offset2bag, "offset2bag", 1);
+    checkScalarType("embedding_bag", offset2bag_arg, kLong);
+    checkContiguous("embedding_bag", offset2bag_arg);
+    offset2bag_ = offset2bag;
+  }
+
+  if (sparse) {
+    return at::_embedding_bag_sparse_backward(
+        grad, indices, offsets, offset2bag_, bag_size_, num_weights,
+        scale_grad_by_freq, mode, per_sample_weights);
+  } else {
+    return at::_embedding_bag_dense_backward(
+        grad, indices, offsets, offset2bag_, bag_size_, max_indices_, num_weights,
+        scale_grad_by_freq, mode, per_sample_weights);
+  }
+}
+
 Tensor _embedding_bag_dense_backward_cuda(const Tensor &grad_, const Tensor &indices,
                                    const Tensor &offsets,
                                    const Tensor &offset2bag,
diff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml
index 724ff917c5..e282bdf96b 100644
--- a/aten/src/ATen/native/native_functions.yaml
+++ b/aten/src/ATen/native/native_functions.yaml
@@ -1101,6 +1101,9 @@
     CUDA: _embedding_bag_cuda
 
 - func: _embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights) -> Tensor
+  dispatch:
+    CPU: _embedding_bag_backward_cpu
+    CUDA: _embedding_bag_backward_cuda
 
 - func: _embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights) -> Tensor
 
diff --git a/aten/src/ATen/native/sparse/SparseTensorMath.cpp b/aten/src/ATen/native/sparse/SparseTensorMath.cpp
index 16573983cb..14ca722031 100644
--- a/aten/src/ATen/native/sparse/SparseTensorMath.cpp
+++ b/aten/src/ATen/native/sparse/SparseTensorMath.cpp
@@ -11,6 +11,7 @@
 #include <ATen/native/BinaryOps.h>
 
 #include <TH/THBlasUtils.h>
+#include <TH/generic/THTensorApply.hpp>
 
 namespace at { namespace native {
 
@@ -535,27 +536,125 @@ SparseTensor& add_out_sparse_cpu(SparseTensor& r, const SparseTensor& t, const S
 // add(Tensor, SparseTensor, Scalar)
 //    formerly known as spcadd
 // --------------------------------------------------------------------
-
 template <typename scalar_t>
-void add_dense_sparse_worker_cpu(Tensor& r, Scalar value, const SparseTensor& sparse, const Tensor& indices, const Tensor& values) {
+void add_dense_sparse_worker_non_hybrid_cpu(Tensor& r, Scalar value, const SparseTensor& sparse, const Tensor& indices, const Tensor& values) {
   auto indices_accessor = indices.accessor<int64_t, 2>();
   auto values_accessor = values.accessor<scalar_t, 1>();
 
   scalar_t* r_ptr = r.data_ptr<scalar_t>();
   scalar_t cast_value = value.to<scalar_t>();
-
+  auto sparse_dim = sparse.sparse_dim();
+  std::vector<int64_t> result_stride(sparse_dim);
+  for (int64_t d = 0; d < sparse_dim; d++) {
+    result_stride[d] = r.stride(d);
+  }
   at::parallel_for(0, sparse._nnz(), 0, [&](int64_t start, int64_t end) {
     for (auto k = start; k < end; k++) {
-      int64_t index = r.storage_offset();
-      for (int64_t d = 0; d < sparse.sparse_dim(); d++) {
-        index += r.stride(d) * indices_accessor[d][k];
+      int64_t index = 0;
+      for (int64_t d = 0; d < sparse_dim; d++) {
+        index += result_stride[d] * indices_accessor[d][k];
       }
       r_ptr[index] += cast_value * values_accessor[k];
     }
   });
 }
 
-Tensor& add_out_dense_sparse_cpu(Tensor& r, const Tensor& dense, const SparseTensor& sparse_, Scalar value) {
+template <typename scalar_t>
+inline void add_dense_sparse_worker_cpu(Tensor& r, Scalar value, const SparseTensor& sparse, const Tensor& indices, const Tensor& values) {
+
+  // Get the dense dimension element numbers of hybrid sparse tensor
+  int64_t values_dense_size = values.stride(0);
+  AT_ASSERT(values.is_contiguous());
+  scalar_t* v_ptr = values.data_ptr<scalar_t>();
+
+  scalar_t* r_ptr = r.data_ptr<scalar_t>();
+  AT_ASSERT(r_ptr != nullptr);
+
+  auto indices_accessor = indices.accessor<int64_t, 2>();
+  scalar_t cast_value = value.to<scalar_t>();
+  auto sparse_dim = sparse.sparse_dim();
+  std::vector<int64_t> result_stride(sparse_dim);
+  for (int64_t d = 0; d < sparse_dim; d++) {
+    result_stride[d] = r.stride(d);
+  }
+
+  at::parallel_for(0, sparse._nnz(), 0, [&](int64_t start, int64_t end) {
+    for (auto k = start; k < end; k++) {
+      auto r_index = r_ptr;
+      for (int64_t d = 0; d < sparse_dim; d++) {
+        r_index += result_stride[d] * indices_accessor[d][k];
+      }
+      auto v_index = v_ptr + k * values_dense_size;
+      PRAGMA_SIMD
+      for (auto v = 0; v < values_dense_size; v++) {
+        r_index[v] += cast_value * v_index[v];
+      }
+    }
+  });
+}
+
+template <typename scalar_t>
+inline void add_dense_sparse_worker_non_coalesced_cpu(Tensor& r, Scalar value,
+    const SparseTensor& sparse, const Tensor& indices, const Tensor& values) {
+
+  // Get the dense dimension element numbers of hybrid sparse tensor
+  auto values_dense_size = values.stride(0);
+  AT_ASSERT(values.is_contiguous());
+  scalar_t* v_ptr = values.data_ptr<scalar_t>();
+  AT_ASSERT(v_ptr != nullptr);
+
+  scalar_t* r_ptr = r.data_ptr<scalar_t>();
+  AT_ASSERT(r_ptr != nullptr);
+
+  scalar_t cast_value = value.to<scalar_t>();
+  auto sparse_dim = sparse.sparse_dim();
+
+  auto indices_accessor = indices.accessor<int64_t, 2>();
+  int64_t result_length = r.size(0);
+  std::vector<int64_t> result_stride(sparse_dim);
+  for (int64_t d = 0; d < sparse_dim; d++) {
+    result_stride[d] = r.stride(d);
+  }
+
+  auto sparse_nnz = sparse._nnz();
+  int32_t max_threads = at::get_num_threads();
+  max_threads = (result_length < max_threads) ? result_length : max_threads;
+  int64_t avg_chunk_down = result_length / max_threads;
+  int64_t chuck_size[max_threads];
+  for (auto i = 0; i < max_threads; i++) {
+    chuck_size[i] = avg_chunk_down;
+  }
+  //make chunk balance among threads as 211
+  for (auto i = 0 ; i < result_length % max_threads ; i++) {
+    chuck_size[i] += 1;
+  }
+  int64_t chuck_sum_size[max_threads + 1];
+  chuck_sum_size[0] = 0;
+  for (auto i = 1; i < max_threads; i++) {
+    chuck_sum_size[i] = chuck_sum_size[i - 1] + chuck_size[i - 1];
+  }
+  chuck_sum_size[max_threads] = result_length;
+  at::parallel_for(0, max_threads, 0, [&](int64_t start, int64_t end) {
+    for (auto k = start; k < end; k++) {
+      int64_t chunk_begin = chuck_sum_size[k];
+      int64_t chunk_end = chuck_sum_size[k + 1];
+      for (int64_t n = 0; n < sparse_nnz; n++) {
+        int64_t chunk_offset = indices_accessor[0][n];
+        if (chunk_offset >= chunk_begin && chunk_offset < chunk_end) {
+          int64_t r_offset = result_stride[0] * chunk_offset;
+          for (int64_t d = 1; d < sparse_dim; d++) {
+            r_offset += result_stride[d] * indices_accessor[d][n];
+          }
+          scalar_t* v_index = v_ptr + n * values_dense_size;
+          auto r_index = r_ptr + r_offset;
+          THBlas_axpy<scalar_t>(values_dense_size, cast_value, v_index, 1, r_index, 1);
+        }
+      }
+    }
+  });
+}
+
+Tensor& add_out_dense_sparse_cpu(Tensor& r, const Tensor& dense, const SparseTensor & sparse_, Scalar value) {
   AT_ASSERT(!r.is_sparse());
   AT_ASSERT(!dense.is_sparse());
   AT_ASSERT(sparse_.is_sparse());
@@ -571,19 +670,15 @@ Tensor& add_out_dense_sparse_cpu(Tensor& r, const Tensor& dense, const SparseTen
   TORCH_CHECK(canCast(commonDtype, r.scalar_type()), "Can't convert result type ", commonDtype, " to output ", r.scalar_type(), " in add operation");
 
   r.resize_as_(dense);
-  SparseTensor sparse = sparse_.coalesce();
 
-  LongTensor indices = sparse._indices();
-  Tensor values = sparse._values();
-  int64_t nDim = dense.dim();
-  int64_t nDimI = sparse.sparse_dim();
-
-  if (sparse._nnz() == 0) {
+  auto sparse_nnz = sparse_._nnz();
+  if (sparse_nnz == 0) {
     if (!is_same_tensor(r, dense)) r.copy_(dense);
     return r;
   }
 
-  Tensor valuesBuffer = values.to(commonDtype);
+  int64_t dense_dim = dense.dim();
+  int64_t sparse_dim = sparse_.sparse_dim();
   Tensor resultBuffer = r;
   if (r.scalar_type() != commonDtype) {
     resultBuffer = dense.to(commonDtype);
@@ -591,22 +686,51 @@ Tensor& add_out_dense_sparse_cpu(Tensor& r, const Tensor& dense, const SparseTen
     resultBuffer.copy_(dense);
   }
 
-  // accessors rely on nnz test
-  if (nDim > nDimI) {
-    auto indices_accessor = indices.accessor<int64_t, 2>();
-    for (int64_t k = 0; k < sparse._nnz(); k++) {
-      Tensor dstBuffer = resultBuffer;
-      for (int64_t d = 0; d < sparse.sparse_dim(); d++) {
-        dstBuffer = dstBuffer.select(0, indices_accessor[d][k]);
-      }
-      Tensor srcBuffer = valuesBuffer.select(0, k);
-      dstBuffer.add_(srcBuffer, value);
+  Tensor values = sparse_._values();
+  bool sparse_is_coalesced = (sparse_.is_coalesced() || sparse_nnz == 1);
+  bool result_is_contiguous = ((r.storage().data() != nullptr) && resultBuffer.is_contiguous());
+  bool value_is_contiguous = values.is_contiguous();
+  bool is_contiguous =  (result_is_contiguous && value_is_contiguous);
+
+  if (is_contiguous && sparse_is_coalesced) {
+    LongTensor indices = sparse_._indices();
+    Tensor valuesBuffer = values.to(commonDtype);
+    if (sparse_dim == dense_dim) {
+      AT_DISPATCH_ALL_TYPES(
+          commonDtype, "add_dense_sparse_non_hybrid", [&] {
+            add_dense_sparse_worker_non_hybrid_cpu<scalar_t>(resultBuffer, value, sparse_, indices, valuesBuffer);
+          });
+    } else {
+      AT_DISPATCH_ALL_TYPES(
+          commonDtype, "add_dense_sparse_hybrid", [&] {
+            add_dense_sparse_worker_cpu<scalar_t>(resultBuffer, value, sparse_, indices, valuesBuffer);
+          });
     }
-  } else {
+  } else if (is_contiguous && (sparse_dim > 0)) {
+    // Handle sparse is not coalesced
+    LongTensor indices = sparse_._indices();
+    Tensor valuesBuffer = values.to(commonDtype);
     AT_DISPATCH_ALL_TYPES(
-        commonDtype, "add_dense_sparse", [&] {
-          add_dense_sparse_worker_cpu<scalar_t>(resultBuffer, value, sparse, indices, valuesBuffer);
+        commonDtype, "add_dense_sparse_worker_non_coalesced", [&] {
+          add_dense_sparse_worker_non_coalesced_cpu<scalar_t>(resultBuffer, value, sparse_, indices, valuesBuffer);
         });
+  } else {
+    // Slow path for non-contiguous values and output
+    SparseTensor sparse = sparse_.coalesce();
+    LongTensor indices = sparse._indices();
+    values = sparse._values();
+    Tensor valuesBuffer = values.to(commonDtype);
+    auto indices_accessor = indices.accessor<int64_t, 2>();
+    at::parallel_for(0, sparse._nnz(), 1000, [&](int64_t start, int64_t end) {
+      for (auto k = start; k < end; k++) {
+        Tensor dstBuffer = resultBuffer;
+        for (int64_t d = 0; d < sparse_dim; d++) {
+          dstBuffer = dstBuffer.select(0, indices_accessor[d][k]);
+        }
+        Tensor srcBuffer = valuesBuffer.select(0, k);
+        dstBuffer.add_(srcBuffer, value);
+      }
+    });
   }
   if (r.scalar_type() != commonDtype) {
     r.copy_(resultBuffer);
diff --git a/aten/src/TH/generic/THTensorApply.hpp b/aten/src/TH/generic/THTensorApply.hpp
index b5aa71aeae..c5144a2b46 100644
--- a/aten/src/TH/generic/THTensorApply.hpp
+++ b/aten/src/TH/generic/THTensorApply.hpp
@@ -78,7 +78,7 @@ if (std::isnan(val)) break;
 #else
 #define PRAGMA(P)         _Pragma(#P)
 #define PRAGMA_IVDEP      PRAGMA(ivdep)
-#define PRAGMA_SIMD       PRAGMA(simd)
+#define PRAGMA_SIMD       PRAGMA(omp simd)
 #endif
 
 #define TH_TENSOR_APPLY2_PARALLEL(SIZE, CONTIG1, CONTIG2, TYPE1, TENSOR1, TYPE2, TENSOR2, CODE, THRESHOLD) \
-- 
2.17.1

